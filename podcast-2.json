{"podcast_details": {"podcast_title": "Chai Time Data Science", "episode_title": "Top Kagglers Panel on Best Practises for Training Models", "episode_image": "https://d3t3ozftmdmh3i.cloudfront.net/production/podcast_uploaded_nologo/1929955/1929955-1576774519995-8f09a9ba4444f.jpg", "episode_transcript": " Hey, everyone. I just did a live stream. That's why my throat is a bit dried up. I have been live streaming every week, every weekend, in fact, to the Chai Time Data Science YouTube channel. So, if you want to be a part of the study groups I've been doing, you can go and check those out. This, if you've seen the heading, is a panel of absolute Kaggle legends. I have hosted pretty much the, I think a large part of the H2AI team, the highest of the highest rank Kagglers. We talk about different opinions of theirs on different best practices. We talk about very fine things like should you use early stopping? What learning schedulers do they find useful? What data augmentations do they use? How do they do stacking in their model practices? So, I hope you'll enjoy this. I hope you'll find this very practical. I really wanted to do this panel. I was really excited about it. I did say in the opening, but the panel by themselves had about 87 collective gold medals. So, it's a group of absolute legends. I won't stand in your way. I'll just point one thing out if you plan to listen to these interviews every week. On Thursdays, I'll be publishing the older ones that I had livestreamed. And on Sundays, I'll publish the new ones. So, if you haven't watched the livestreams or if you've been waiting for that, at least for the next 10 weeks, I'll keep publishing the old ones. If you want to skip those, in the intros, I will mention that this is an old interview from whatever date it is. This livestream just happened a few days ago. So, I hope you'll find this useful. I hope you'll enjoy. Here's the panel. Alright, let me make sure I'm live. I sometimes mess up the settings. So, if the audience is just looking at me, there are wonderful people waiting backstage and just making sure everything is okay. It takes a minute sometimes. So, please bear with me if you're still looking at my silly face. It didn't take a minute. Awesome. Hey, everyone. I am super excited to be having Chai built on a special hydrogen stove. Do not try this at home. This is done by professionals. But I'm excited to have the best Kagglers as a panel today. We'll be debating best practices for training ML models. I was counting the medals. Some of them have teamed up. But still, regardless, our panel together has accumulated 87 gold medals on Kaggle competitions. They've built incredible products at H2O.ai. They are the real deal. We'll be learning about best practices for different things. I'll try to start some debate, maybe start a fight if I can. Along the way, I'll show you Hydrogen torch, which is a no-code deep learning tool built by the panel. You'll also see how it is really easy to do these things in that. So, I know you don't want to hear more of me. I'll start introducing the panel today. I'll go in the alphabetical order. We have Dimitri Godiv, who's the Director of Data Science and Product. His highest rank has been five on the competition's leaderboard. Dimitri, thanks for joining us. Thank you, Sayam, for having me. Hi, everyone. Awesome. I'll keep going in order. Next, we have Gabor Fadur, who's a double GM. His highest rank was four. Currently, he's a Principal Data Scientist. Welcome, Gabor. Hi, guys. After that, I didn't do this on order, but it appears the team Hydrogen, you might know them from leaderboard, is in the next serial order. We have Pascal Pfeiffer, Principal Data Scientist, and his best rank was five. I'm sure he'll climb further up with more medals. Thanks for joining us, Pascal. You muted. Thank you, Sayam. Thanks for having me. Also, this was really hard to convince him, but I got some help from Philip because Pascal usually doesn't reveal his face. So this is like a very special moment for all of us in that regard. After that, we have the current king on the leaderboard rankings, Philip Singer, who's a dual Grandmaster and Senior Principal Data Scientist at H2O. Philip, this is the ninth interview you're doing with me. Thanks for doing another one. Good being here again. Hi, everyone. And finally, we have Yawen. Some people might know him as BS. I was one of the fortunate ones who've been following him since. His highest rank has been 14, and he's a Principal Data Scientist at H2O. Thanks for joining us, Yawen. Thank you. Hey, everyone. So I have this very important question to start with. What's the best seed? Not using any seed at all, I would say. Just keep it unfixed and run your experiments. You'll see how the range of all the experiments go and how all your scores are distributed. So you kind of see how stable or unstable your validation set is. And you could even make use of binning and test binning by running different seeds of the same model, same routine. So yeah, it kind of makes sense to not fix the seed at all, I would say. But do you do seed averaging after that? Or like towards the final ensemble once you like figure out what's working best? Yeah, exactly. Like I said, so similar to to ensembling, you can do something like binning, which I call the average of multiple seeds for the same model or same training routine. Okay. No one else wants to share this special seed. So I assume I'll continue regardless. I know this is like something maybe maybe you all are opinionated or maybe you all share the same opinion. But early stopping, do you do prefer using it in your experiments? Yes or no? And I know, Philip, you have a strong opinion. So maybe we can start with you. Yeah, I dislike early stopping a lot. So I will never do it. Maybe sometimes in like light GBM models, maybe just to get a feeling. But otherwise, I would always encourage to optimize the number of epochs across all folds if possible. And because early stopping is is is leaky in terms of overfitting to the validation data set. But this is just in a nutshell, there is a longer answer to it. But to answer your question, I don't like early stopping too much. I used to be I used to be a fan of early stopping before. But Philip is trying hard to convince me that it is it is not the things that you need to do. And I guess in the last year, I haven't used it. But before that, it was it was my way to go most for for the classic machine learning and deep learning use cases. But currently, I'm turning to the dark sides. Does anyone use early stopping it? I mean, just as Philip said, I sometimes use it to get some very, very quick results with light GBM or other boosting methods. So this is probably the only time that I ever use early stopping. And other than that, I just feel it. It's leaking information from your validation set to the to the training pipeline. So this is definitely something I would I would like to avoid at all costs. So yeah, just as Philip, I never use it. Same for me. Basically, run it maybe once, twice, whenever you change parameters a lot, just to understand what's approximately the optimal number of estimators for light GBM or number of epochs. And then that's basically become that becomes a hyper parameter to tune. Yeah, same. So in the beginning of the competition, so to have a kick kick baseline, I usually use it. But after a while, when I have more time to form long running experiments, then it's safer to change to fix epochs for all models. And and maybe if I if I can, I think someone on this group posted it a while ago that there was someone on Twitter saying, you wouldn't tune any other hyper parameter differently across different faults. And I think this is quite a good analogy why you wouldn't tune something that is different per fault and early stopping usually can be very different per fault. So you would sometimes stop at the third epoch, fifth epoch. And there is another big benefit of not doing it, which I think is a different topic to talk about, which is retraining on the full data. But if you don't do it, it's also easier, easier to do that. Okay, I was expecting some some some form of fight, but maybe maybe I'll keep maybe we'll have a GM fight in this panel. But early stopping is a no for everyone. Thanks for that. The next question is, there's also the strong debate, especially for people from fast a group, they really like the learning rate scheduler that fast a offers. Do you have a strong opinion there? Do you prefer just the cosine learning rate? Or what's what's your preference? And what is what is your favorite scheduler at fast AI group? There's an inbuilt one. I don't remember what what the details are. But you just call LR.fit and it uses one implemented by Leslie Smith. Okay. Yeah, for me, I guess I'm always using just because cosines scheduler. So it is first and probably the only option I'm trying. Previously, when I was a fan of early stopping, I was also using it with with a warm restarts, right? So it means that you're you're doing one cycle, then you're making another one and then you're kind of you can average maybe multiple checkpoints or select the best one. But yeah, but currently I don't I don't even try other other scalers. I guess it's a moment. And sorry, just this for the audience. What I was doing right now was I was screen sharing to hydrogen torch and just walking through the different settings there and showing the different shaders in there. Does anyone else want to share what they use as a baseline? Okay, I mean, I also always use cosine decay only either with a short warm up or not. And I think this also fits very well to the to the discussion with early stopping because if you try that the last epoch is the best one, you try to get a nice a nice decay on the learning rate and try to be decaying towards the end. And also a couple of years ago when I started with deep learning, and it was a bit more popular back then is to do stuff like automatic decaying of the learning rate on the plateau or something, right? So you run one epoch, you see, okay, it doesn't improve for one or two epochs, you automatically reduce it. But all of this is very overfitted to the validation data set, you cannot properly do it for training on a full data. So I think, I think in the end, actually, I'm, or most of us, I believe, are using cosine decay. Linear decay works pretty much the same, but just some some some steady decay towards towards the end of the force. I assume Philip sort of shared it for everyone. Pascal, please go ahead. Yeah, I think it even fits a little bit to the previous question. Because with higher learning rates, some more shaky metrics tend to go up and down quite a lot between each epoch. So you may run into some some local optimum, but it's just very, very specific to your validation data and never would, would be able to do the same performance on any unseen data. So I think having a very low learning rate at the end makes most sense in these for these cases. But I also think that there are some use cases where cosine may not be ideal. And that would be like training super large language models on on a huge amount of of GPUs or TPUs. And in these cases, yeah, I also see that that very constant learning rate can be beneficial, but this is actually a super low learning rate in this case. I was just showing how the cosine learning rate behaves in this graph. And this is basically for fine tuning. And this is mostly the way to go. Here's a question by CrowDog. I'm sure you all know who is shout out to CrowDog. He's asking do you use stochastic stochastic weight weighted averaging? Is it weight averaging weighted averaging? Probably no. I guess I guess I guess I have tried it a couple of times, but it never worked for me than just selecting the last best checkpoint. So yeah, and again, it aligns with this idea right that we are just taking we're trying to get the best performance is the last epoch without early stopping and yeah, so you should work better for me. Okay, thanks. Thanks for sharing. Yeah, I think on calendar is sometimes the flavor of the month, right? So some obscure technique works in some competition. Suddenly, everyone thinks this is like the best thing to use across all competitions. And I think stochastic weight averaging was used somewhere and then people like to try it again, specifically in NLP, there's a lot of different techniques out there in order to try to regularize the training a bit better. But in the end, those are all just techniques to regularize. And usually from my experience, you can just use simpler methods and it works equally well and you don't have extra runtime. So there is a lot of stuff very specifically tailored towards some tasks and and and yeah, just because it works for someone doesn't mean it will always work. And at the same time, it doesn't mean there is not something simpler that will have the same effect and also work. So sometimes I try it when I see something new popping up. But usually I feel like there's a lot of a lot of noise in these techniques out there and you don't need to use all of them. Thanks, thanks, Philipp. The next question I had was, can you please share your base practices? You sort of pointed towards it, but for warm up epochs. And if I show the default settings, it does show everyone's preference towards cosine because that's always the default option in the menu for hydrogen torch. But best practices for warming up the learning rate. No one wants to share. I can start. So basically for me, warm up works sometimes. Sometimes. But the problem is that it is hard to tune. So usually once you have a fixed number of epochs, I feel it is kind of counterintuitive to tune it together with number of epochs. And also, like their peak of the learning rate is different, right? So you have to adjust the number of epochs and I can't tune it well. So maybe that is why it is not working for me. But sometimes, sometimes it is useful. And I guess in one of the recent NLP competitions, we even used like two epochs. And one epoch was warm up. So actually it was like one epoch warming up 50% of training. And then 50% of training, it was like just going down on the cosine schedule. So sometimes it is helpful. Thanks. The hydrogen team hydrogen has won many, many competitions together. And I can't even remember which one Yawian is referring to because they meddled in so many competitions together. Does anyone have any strong opinions on this? Okay, I can keep going in order of my questions. The other topic, and I assume this is really from a beginner perspective, because these things are heavily discussed throughout different courses and stuff like that. But any strong opinions on optimizers you'd like to use while training? Adam, Adam is enough. Let's just use Adam. Adam W and you're good is my opinion. Anyone else have an opinion on that? Yeah, I think again, it depends on your intuition. So if you used to work with SGD, for example, and you know, they're like best schedulers and best learning creates that are working for this optimizer, it might be better for you to continue using SGD. But yeah, but I guess the way to go is Adam W now everywhere, again, only for fine tuning tasks. So mostly we're talking here about fine tuning tasks. And I think it is it is the best best choice here. At least it makes it very, very easy to to get some initial good results. I mean, it doesn't or it allows you to set a large range of learning rates and it would still work on most backbones at least, which is not the case for SGD. So it needs to be tuned a lot better to or not a lot more to get the same results probably. And again, I was pointing out that I'm sort of trying to reverse engineer how hydrogen torch was built. But the default again was Adam W. And you can see the opinions here as well from the panel. But these are the general options. And the next question I have is really visible here. Many people talk about differential learning rates. So what I'm trying to solve here is a segmentation problem. And as you can see, there are many different problems supported. I just chose this as an example. But any thoughts on differential learning rate? And for the audience, this is the act of applying different learning rates to different layers, because you assume that the initial years are already trained, the outer layers are less trained. So differential learning rates is the question. Yeah, I can answer it again. So I think differential learning rate is an easy technique that can be very helpful, specifically for certain tasks. So basically, it means that you give different types of learning rates to different parts of the network. And usually, usually the easiest difference is that you give the backbone a different learning rate and the head, for example, a different learning rate or the encoder and decoder in this case. And different to what many people think, a higher learning rate usually means less overfitting. So you because it cannot make two fine grained jumps in to go into a very specific overfitting area. And basically, what usually works well is to give the head of the network a higher learning rate. And sometimes it is even easier doing this from the start because switching the backbones is easier because backbone sometimes needs very different learning rates. So let's say you use an efficient net versing versus a transformer based backbone model, then you can just keep the head learning rate constant and just change the learning rate of the backbone and it sometimes makes it easier to try different backbones. And as I said, the regularizations of some problem types even need it. So in metric learning, it is super important in things like semantic segmentation here, it can be very important. And sometimes it is really, really necessary and sometimes more a tuning thing. But yeah, a good technique to play with. Thanks for sharing that, Philipp. Maybe I'll ask Gabor if he has anything to add. Haven't used it yet. So usually I try to keep it simple, especially even the learning rate. So I'm not a good example for finding the best models. So whenever I have time, and I mean, we have enough inference time for Kaggle competition, usually I go with a blend of army of weaker models. So not necessarily the stronger ones. It doesn't necessarily a successful technique, but it usually helps to build a more stable model. But of course, it would be insane to put into in production. So it only works for CSV like Kaggle competitions or smaller datasets where you have enough inference time. Thanks Gabor. Pascal, I saw you unmuting if you still have to. Yeah, I mean, differential learning rate usually also helps in the beginning of training. So similar to that warm up we mentioned. So if you have a very biased target, for example, you would probably give the head a higher learning rate and then it adapts quickly without putting too much gradients on the backbone within the first steps. And also, what has come with the rise of the transformer models, they usually need a very, very small learning rate to work well. And if you still want a larger head, for example, with two layer dense or two dense layers or something like this, you may want to have an order of magnitude higher learning rates there. Thanks, Pascal. Awesome. So while I was trying to showcase different things, one thing I really enjoy about the fact of using hydrogen torch every day at work, I get to use this just for experiments is the fact that even to implement differential learning rates, like I could mess up in so many ways. One time I'd set a global name for LR. And that was messing up differential learning rates. But here, everything is taken care of. And I can just select all of these options. There are also very fine things taken care of. So if you want to like not painfully experience the last batch giving a weird error after two days of training, this setting is already taken care of. So usually the last batch is always dropped without too much thinking. So while I keep weaving in and out of that, I would like to also shift the conversation to its now computer vision specific models. I know people also have very strong opinions on data augmentation. And maybe I can start a fight with this. But your opinions on favorite data augmentations for computer vision problems. Mostly try everything that you can think of and check your score, I would say. I mean, you can't have favorite augmentations because these are just super specific to the actual problem you're trying to solve. And in some cases, augmentations don't make sense at all from a physical standpoint. In some cases, they make perfect sense, but they still don't yield better results. So I think it's just a matter of trying out anything that you can come up with. And as you've just shown in hydrogen, we have a couple of predefined settings there, but you can also go ahead and customize anything. Usually, I'm creating kind of a simple notebook with augmentations pipeline for each problem. And you're trying to kind of generate a couple of examples of images from the data set and apply it with the probabilities you want to adjust. And usually I'm just clicking 10 times to see what are the different images I'm getting, augmented images I'm getting from the original image. And usually it gives you an idea what is the best set of augmentations for your specific problem type. Or how would you like it to have at least from your point of view. And on top of it, all this mixed augmentations, right? So in hydrogen, we have cut mix and mix up. So yes, for example, here is an example of cut mix where we are pasting one image into another with different parts. And this mix of mutations are really working well for almost all the tasks, I would say. So sometimes it is mix up, sometimes it is cut mix, but at least one of them is usually working. Yeah, also, my personal experience, I tend not to have some sort of insights or guts feeling what's going to work this time, what's not going to work. So it's all trial and errors, except for some cases when like obviously flipping would work or obviously flipping would ruin things. So it's the matter for me to of iterating. So starting with something simple, either no augmentations or something that would obviously work and then try things that would add value. And typically most interesting ones are exactly the strange ones like cut outs, cut mix, mix ups, all the augmentations that do something with an image, which is for me very difficult to predict whether it will help model or not. So just matter of trying, preferably one by one and try to grasp kind of the feeling what works and what combinations of augmentations work. Between the panel, Dimitri and Philip's team had won a competition that actually was and their model was used in real world in NFL. I might mess up the details, but it was actually impactful in the real world and Yahoo is widely known as one of the best in computer vision competitions. I was curious to extract details from them. Moving on to the next one. People heavily also debate the backbones and architectures for segmentation. So do you just use unit, unit plus plus, or is it also specific to the problem in your opinion? I can start. So yeah, so usually I'm for segmentation, I'm using like unit and unit plus plus, and I guess I'm already like far behind the current state of the art. So probably there are lots of different new architectures, but for the majority of their semantics implementation use cases, especially if you have a low number of classes, like under 10 classes, for example, or under a dozen classes, classes, unit is working nicely out of the box. And advantage here, for example, in hydrogen towards that we have also selection of backbones, right? So apart from having just this unit architecture, you can select almost any backbone in intent from team library. And it allows you to also squeeze far more performance even using this kind of like old school unit unit architecture. Yeah, and let me put you on the spot. I've never heard of regnets. Have you ever used this in a competition? This is the first time I heard this model. Could you scroll on? Actually, I have I have like read a paper during our last competition, I guess it was DFL competition, like this Bundesliga competition in September. And I've read the paper. And their idea, I guess, of the paper was that it is very shallow networks that works, that work works much faster than efficient nets. But and it produced pretty solid results. I guess we got a pretty good baseline with regnet. But after like in the end of the competition, efficient net V2 was was the winner. So it was it was good, but not not as good enough. Thanks. Thanks for sharing that. Yeah, I think for both for both what your handset for semantic segmentation, object detection, and so on, I think the interesting aspect that you can try when when when training models is to is to try different backbones and and a lot of the more popular frameworks, I would say are very limited in the choice of of of backbone. So let's let's take object detection, you have YOLO with like a very limited, very shallow backbones, you have efficient debt, which only has efficient nets, obviously, as backbones. And and and what what we did in the past, and what we also have here in hydrogen is that you actually can can can can select, like all the team or most of the team backbones, for example, for for object detection for RCNN based and or F cost based techniques, and also for semantic segmentation. So I think there is really a lot of a lot of room to to to tune this kind of two stage approaches with different backbones. And yeah, so so so I think that that that that is valuable. And there's, from my experience, not a lot of pre built frameworks that can do that easily. So I think that that's that's where we have a lot of cool stuff specifically in hydrogen also. And in the meantime, what I did was just just to compare different backbones and different architectures, I also ran a grid search. And that's also possible just through the menu as I was showcasing. Okay, I'm sorry, I got distracted looking at the questions. And for the audience, please keep the questions coming. I am looking at them as you saw me distracted, I lost them towards the end. So if you're watching, please keep asking the questions in the chat. Shifting the conversation again towards NLP. Any strong opinions on data augmentations in NLP that you really enjoy or you're really against? Usually, when I'm starting an NLP competition, I'm always excited that yes, at this time, I will try some fancy mutations and they will work for me. But it is either they don't work at all, or I don't have time to try it. And it is a priority. So usually it is my experience. But but yeah, I'm really excited about this like back translation augmentation, right? That you translate to one language and translate back. I have never tried it. But it looks like something that might work. And the only things that worked for me was their masking augmentation, right? Where we adjust like randomly masking their tokens. So it's just just as a classical augmentation is the only ones that worked for me before. Same here, probably. So the only thing that actually really got a got a boost in any competitions that I took place in was dropping either single tokens or words or even complete sentences or larger blocks of tokens. So this is kind of the thing that usually works. But there is yet this other technique like MLM pre training that you can do, which kind of achieves a similar thing, I would say. But it's even usable on a on a data set on an unseen data set on unlabeled data set. So yeah, with with regard to augmentations, it's usually just the dropout and other techniques, such as replacing words with similar words or this this translation into another language and back translation never really got me any boost. So it's usually just the same score, but it takes a lot longer to train. Sorry, please, please. No, I just wanted to say that's that's one of the reasons why I like tuning computer vision models more because NLP is very boring to tune because there's like no augmentation at work is I haven't seen any single augmentation that works apart from your masking tokens. Because the the the the models are way stronger pre trained than computer vision models. So they have a way better understanding of of different different settings is at least what I see. And yeah, so so I haven't seen seen anything useful. And of course, here or there is something maybe right, but definitely nothing that you could say is as popular as mix up or cut mix or simple things like flipping even are useless, right. So yeah, it's definitely, it's definitely an unfortunate lack in NLP, or maybe a fortunate I don't know that that that augmentations are not working so well. You actually know that you're saying mix up is not working. So if you have a very, very specific problem where you could exchange sentences with each other, which each other from another batch or you've been another sample that does work in some cases, but it's it's as you said, there's not much to gain from because these, these models tend to be very, very good already pre trained. So that's also the point why you usually need only a couple of epochs in NLP versus quite a few epochs in in CV competitions. Yeah, I have tried cut mix based mixing. I have tried mix up of the embeddings, right, that is one technique you can do. But nothing really has ever helped me. So maybe I'm doing it wrong. But even if it if it if it makes sense, and this matches what Dimitri said before, sometimes, sometimes I even try to think, okay, this augmentation has to make sense, right. And when you train a model, it doesn't make any sense any longer, or it doesn't help. So yeah, I hope there will be something that is as popular as mix up. But for now, I don't see it. Or you can also tune the batch size. If you're training transformer models is super sensitive to that. Yeah, I mean, they're way, way more low hanging fruit specific fruits in specifically in NLP to tune it's a bit more sensitive to learning rate than computer vision models, because it's transformer based, it's more sensitive to E boss and song. So I think I think there there is more room versus playing with any fancy augmentation that you can think of, specifically in given limited time scenarios. Awesome. I'll continue in my questions list. So what I've done next is I've loaded up the last feedback competition where Philip I think competed solo because he wasn't leave. And as you can see, it also supports text regression problem among everything else. But the topic I was coming to was I know Philip, you said you have a strong preference towards gem pooling. But again, is this also something you all experiment with or any opinions on pooling layers you like to have in your backbones? Yeah, I usually use always jam. It's a bit of a lazy version, because it's kind of an automatic blend of max and average pooling in a way. In NLP, I will try class pooling for sure, because it's the most different one to the jam. And but but usually I actually have a strong preference towards jam pooling nowadays. And and mostly use that as a default. But I don't think it's too important. Honestly, I think with average pooling, you can it's pretty much the same. Also class pooling, it's class pooling is maybe the most different one, I would say. But in general, pooling, I don't think is the most important thing to jam. And sometimes for NLP, maybe depends a bit on the problem, right. So if you'd like to have a global context from there, from the whole sequence, you might get you might you might want average pooling across all the tokens. But if you're like interested in the like just the CLS token for classification, like sentiment analysis, whatever some some simple classification, it might be better to use CLS token. But yeah, but usually it is not not not not that much of a difference. I mean, maybe I can I can I can relax that statement by us a little bit because there is some room to be innovative with pooling, I think in NLP. Specifically, if you have like longer text, and you need to have at certain parts of the text, maybe maybe some pooling. So in the in the second feedback competition that was like that, where you have like different statements inside a long text, and you need to make a prediction for like all of these statements, then you would pick out certain parts of the whole whole whole text and just take the poolings at different points. So so stuff like that, I think there is room for innovation, but in the fundamental functionality of the pooling not. I see some people doing the pooling across like different layers and so on. And they spent weeks on just trying to tune that I think all of this is not too useful. Most of the time. And there there are other things which which you better spend time on. It's also sometimes confusing when you read top solutions, it appears that maybe the pooling layer was like a big factor. But sometimes sometimes competitors aren't telling the secrets. Fortunately, many of the people here on the panel usually end up open sourcing their solutions. So you might might find most of the times I found team hydrogen, you can find their code and it's very easy to digest. I've also asked them during the interviews, why do you do that? But they like sharing a lot. So I'll start moving into audience questions and some general questions now on the I'll try to summarize and Tosh's question. But do you usually average your models across your predictions across different pools? Or do you end up retraining your model, your best forward model? And any best practice you can share it on that? Yeah, this is definitely something that we do. Most of the time is to just to not ensemble k fold models for final prediction, but rather different seeds on retrained on the full full data. So deep learning has a very big impact in terms of the amounts of training data, and usually more data is better. And with k fold, you usually throw away, let's say 20% of the data. Sure, it can be sometimes valuable to ensemble different subsets of the data with each other. So there are definitely some use cases where this is helpful. But I think most of the time, it's better and more stable to rather retrain the model k times on the full data and ensemble the different seeds with each other. And as in the beginning of this, this this this panel, we discussed like early stopping, if you have like a fixed epoch, you just you can take all the settings the same and just retrain on the full data and use it blindly. And I would say 90% of the cases, I at least at least do this retraining for for the final final submission if if I have time. If I don't have time, I can even mix k fold models with with full fit models. I don't see any issue in that. But you would maybe wait it similarly. So if you have, let's say three seeds of full fits, you would 5050 blend this with with an average of five folds or something like this. But in general, this has worked well on the private leaderboard mostly. And this approach is also helpful in terms of saving the time is the inference time, right? So if you're assembling five folds, and you have to run like five inferences, once the full data, it is just one single retrain. And it is better to assemble like five different backbones, let's say, for the same runtime, instead of like assembling five, five folds. So it is it is also helpful for for assembling. Yes, and usually so in in in in a rough estimate is that free seeds full train is roughly usually equalish to five fold blend, I would say roughly. Yeah, and it was it was very counter intuitive for me. So I guess I remember we discussed with Philipp and again, I was using like five folds back then, not the full full data retrain. And it was really like counter intuitive that it has a similar performance as in sampling of five folds. Because from my point of view, it was that if you're like in sampling five folds, it should be better than single model at least more stable. But yeah, but it occurs that it is not not actually true or partly not true. Yeah, but sometimes the output for predictions might help for a problem. So I can only imagine competitions where there was a strong image component. And also some strong tabular or time series like component and two stage models training different CV models and different tabular like models like like GBM and then building a meta model to combine the predictions. Those are the cases where it could have if it's not easy to encode the information into the CV model. Yes, yes, yes. But what works from experience is that you can do these seconds, you mean like second stage models, right or stacker models. So usually works well, or at least it worked fast to train the stacker models on OOF and just apply them to the to the full fits. So that even works most of the time. Yeah, I'm just too cautious to run a blind model with every stage. But yeah, it could be good and stable training pipeline. It could work. But yeah, in those cases, I see the validation score at each each of my local CV. Yes, yes, yes, yes. I mean, even even training the stacker models on in sample predictions mostly works. So you could even do this with the full fit in sample predictions. But yeah, doing it on OOF and then predicting on full fit is usually good enough. But yeah, I see your point. Yes, yes, yes. Thanks for sharing that. And just to point out to the audience, like personally, I'm really enjoying this because I am sure you all know, but Team Hydrogen, for example, there's the special track in the feedback competition with this efficiency track and you're supposed to take to the inference in the fastest time possible and it has to be the most accurate and they've they performed really well in those tracks as well. So like, it's it's we're really getting access to all of this knowledge. And just to point different facts out. Someone had asked if you could please share your favorite debugging strategy. Yeah, I remember that I guess it was some kind of like article saying that it is a good approach to take a small sample of data and then overfit to this data and make sure that your pipeline works. And if you manage to overfit even on the small data. And I tried to like back in the early days, I tried to follow this right to debug your like deporing pipeline, not even going into the code, right, but just the general architecture that everything works in turn. And it never worked for me. So I didn't manage to, to overfit to this small data. And afterwards, I realized that he has probably it is better to just throw in the real data and try and see if it works or not. Right. And afterwards, if you see that something is not working the best, the best way is just to go through your pipeline, right, and debug it in a way that you see what what was the input in this tape and what was the output of this tape and is it expected that does the images look fine? Do there the those outputs of their sizes look fine and so on and so forth. So then it's just a basic debugging as an unusual software code. I mean, other than that, it's usually the shapes that are that are in the wrong order or something like this. So yeah, just just adding some print commands and asserts in the forward already helps quite a lot in the early stages to make sure that you you don't mix up for example, your your width and your height, just because some augmentation was was flipping this. So these are definitely some some things that I usually do in the in the early stages. Yeah, also regarding width and height, making sure you don't use the same value for both helps a lot to to think this through and to not miss miss a flip there. I debug by printing everything in every line. And sometimes they don't even get printed. So that's my favorite strategy. Yeah, I mean, I mean, my colleagues know that I do a lot of prints, and I always forget to remove them in my pull requests. So they have to notify me about that. But in general, I'm also lazy in that sense, I find it the easiest still to print commands. And I think there's another big thing to debugging, which is logging in deep learning. And I think this is even more important than so, if you do all the logging, like what we saw in what you showed before with like, what is your current learning rate? What is your current validation loss training loss, it is very easy to also find problems, right. So if you implement the cosine decay learning rate scheduler, and it and your logging shows a constant one, you know that you have a bug. And otherwise, it is oftentimes very, very hard to figure out this part. Or actually, if you go back to this train data insights, if you implement a new, a new augmentation technique, and you directly visualize your first batch, like we are doing here, and you see, okay, the augmentation is not working as it should be, it's very quick to, to find these issues. And I think that has been very important in the general pipeline that we built in order to do a lot of logging and a lot of automatic visualizations and things like that, which, which, because oftentimes you miss it back because it's not throwing an arrow, right, but it is not doing what it should be. So and that makes it easier to find these issues. Thanks, Philip. Just looking at the chart and also I was teasing my teammate, I said it's someone is he's my teammate and absolutely incredible. I was just teasing him. He's asking, how do you optimize hyper parameters of large models? And if, if you have any, any opinions on that, especially for complex architectures and LB where they're very easy to overfit, especially the larger back larger models. Yeah, I mean, similar to all the problem types, you usually start up with a small, with a smaller subset of training data. So it doesn't take ages to, to find the, to find the issues. And then you would start also with the largest backbone. So if you have D-Berter, for example, as a backbone for, for NLP problems, you wouldn't start off with the largest there is available, but probably a base one, and then start iterating from there and finding a sweet spot where you are and what is a reasonable score to get on this data. And once, once you have that and got a bit of a feeling to, to all the hyper parameters, you can always go to the higher, to the larger backbones to, to more data and all of this. So I think it's always, always important to not use the best or the largest model out there and also not your full data if it's a lot of data. So you can iterate quickly and get to get some, some huge jumps in your, in your metric quickly. Thanks, Pascal. I don't see anyone else unmuting. I'll keep asking the next questions. There's also a usual preference, like you were pointing out, and this was actually discussed in one of our internal calls just, just a few days ago, people, especially novices usually go for the largest model possible. And in computer vision or like generally, when do you prefer simpler models or transformer models? Or is there any time you prefer, let's say, efficient nets or other, other transformer models? I will always prefer the one that works the best, right? So that, that, that's the simple answer. I will try everything. You, you have a preference, you start with something and then you experiment and you experiment and, and you go down, down, down the rabbit hole and either you find something better or you don't. If a efficient net PCO works better than a P3, I will prefer maybe the PCO, right? So it's an experimental science. You try out things and, and, and some things work better. Some things, some things work here better. Some things work there better. But in general, I would say efficient nets in computer vision are kind of across problems, usually the best working and the easiest to tune from experience and, and in NLP it's nowadays the Berta that is clearly working the best. So you have some preference, but you will try other things. And probably in each, in each new competition, it is better to not to have any assumptions right? Is this architectural will work better? So it is always better to, to check all the architectures and select the best one for this specific dataset for this specific competition. So assumptions are usually wrong. Yes. And also don't, don't believe everything that is written on the forums. If someone says, this is the best for, for this, take it with a grain of salt, try it, but it doesn't need to be the best one for, for you. So there is always, always better to try things yourself. So the top public team saying mobile net is best are lying. Yeah, you can, you can, you can, you can believe it or not. Okay. But, but I have, I have, I have fallen myself into this trap, right? So you, you join a competition late, there is a lot of discussion. There is maybe even a high scoring public kernel, which maybe was lucky, right? And you try to replicate this and you try and you try and you try and you, and you try and you can't. And at some point you, you don't understand why, why is it not possible to replicate it? People write it is good, but sometimes there are so many different things coming together and it could be something else that they are doing, which impacts the results and not necessarily the backbone or not necessarily what, what, what they think it, it, it, it is impacting. So it's always better to, to, to try things yourself and also go different directions than what the majority is doing. Thanks Philip. Anyone else want to chime in on this? Okay. I'll take the next audience question, which is if you could share any best practices for training models on large image sizes with class imbalance. Or if I can rephrase that best practices for class imbalances. Yeah. I guess, I guess I can rephrase it. What is the best strategy for RSNA competition? Yeah. I would have understood that question even without the next comment, which, which is hello from RSNA competition. Yeah. I mean, it's typically on Kali that you have large data, right? I don't think there is anything specific. So just browse through the old competitions and try things. We're not, not, not saying too much on this. You had to ask that question more smartly. Sorry. We can't do private sharing. You're a public sharing so to speak. Yeah. But, but, but you realize there is there like a good dependency between the image size and the performance of the model, right? So is there a larger, so at least to some extent the larger the image size, the better the performance. Yeah. So this is, this is a good strategy to start with some smaller images and then increase the sizes afterwards. Thanks, Yavin. As a reminder to the audience, we have five more minutes. So if you have any questions, please keep them coming. I'll, I'll keep selecting them. The next question I'd ask is if you could share any best practices for stacking or second stage models. And we had somewhat discussed this, but if you could share more, more secrets, maybe you can, maybe I can help the person in RSNA virus. Gabor, maybe I can ask you. It depends. So sometimes you need a first stage image detection model just to select different whales. Or if you are talking about large images, you could select the region of interest in those images. But there are a few years back, each and every Kaggle winner submission had a huge crazy stacking model with two or three stages, but I don't think it's common anymore. So I'm not, not that active, but what I see nowadays is a single model or single like architectures could compete with the crazier ensembles nowadays. Thanks Yavuz. Depends on the problem, of course. Yeah. I have quite, quite, quite the same experience. Like back in the days, it was very fancy to have like two, three stage models and throw in five, 10 different modeling techniques into the first stage, few more on the second stage and so forth. But these days, even for tabular competitions, it's more difficult to see two maximum three different approaches. So for stacking, you just usually do something very simple, like weighted average, and that works the best already. I think second stage models can be helpful if the metric is very different to what you can train on basically. So usually then you need to do either some post-processing, let's say aggregate the scores of different subsamples of a person or of a record you have. But here second stage models can make sense to build this in a better way and to directly optimize the metric on a level that you're interested in. But yeah, these crazy stacker models, I'm personally not a fan because they also can be very over-fifty and not so efficient, specifically in deep learning. It's basically not happening too much. But more shallow second stage models I am starting to be more a fan of versus I have been in the past. I think they can be helpful and they have been useful in some previous competitions of ours. Thanks, thanks everyone. Okay, I'll start wrapping up. If there's one last question, I'll try to squeeze it in. So one last call for everyone. In the meantime, I was showcasing as a reminder, hydrogen tossed everyone. And I did prove the point that you can train really high accuracy models. And this is, I think a personal record for me. I've trained about like the highest number of very accurate models in like under an hour, which is like a personal record for me, as you can see. But it's really easy to prototype in all of these experiments, as you can literally see here. If you want to check this out, you can see the first link in the description. I'll also pin it as a comment if you want to check out the software or evaluate it for yourself. And I also established that all of these opinions are also baked into the product. So I don't see any question, any final words for anyone before we wrap up. If I missed anything or if there's anything else you want to share. I'll try to squeeze one question from Alex because he's always a regular to RUNs. He's saying neural network tips for tabular data if anyone has any opinions on that. For me, nothing that would always work. So depending on the tabular data you have, I usually spend a lot of time trying to convert the variables to for the neural network to consume it properly. Or even more typical nowadays, the tabular data is not just tabular data. So there's some structure which we can have starting from kind of time dependency ending with some graph-like connections between the records that you can squeeze in specific types of layers of the neural network and can benefit from that. So apart from that, everything that I would say always works. Thanks, Dimitri. But it shouldn't discourage you from trying. So there is not an easy solution like GBMs in neural networks, but I think there is still potential for some problems to be better solvable with neural networks. So sometimes they can work as I think it's interesting. But even if they don't reach the same performance as like GBM, you just blend it and so very very often it improves the overall score. Just taking a simple average, you get a nice boost. Thanks for sharing that. I'll wrap up now. As a reminder to everyone, this is an absolute legendary panel. I don't know if you don't know anyone. I feel bad for you because you should. And in case you don't and in case you want to connect with them, just check the description. I put the links to everyone's different profiles on the internet. You can always expect them to be sharing such knowledge everywhere on the internet. So you can follow them on Kaggle if you want. You can see all of their Kaggle usernames right now being displayed. You can connect with everyone who is on LinkedIn and most of them are on Twitter as well. So you can find all of the speakers links in the description of this video. Thank you so much everyone for watching and also thank you so much to the panel and all of you for really sharing your knowledge. Okay. Thank you. Thank you. Thanks."}, "podcast_summary": "Thank you for attending the panel discussion. We hope you found the insights shared by the panelists helpful. If you have any further questions, feel free to ask.", "podcast_guest": " Dimitri Godiv", "podcast_highlights": "You're welcome! If you have any more questions, feel free to ask. I'm here to help!"}