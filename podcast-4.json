{"podcast_details": {"podcast_title": "The Gradient: Perspectives on AI", "episode_title": "Gil Strang: Linear Algebra and Deep Learning", "episode_image": "https://substackcdn.com/feed/podcast/265424/s/1354/a07e27ba3d2374446daccc6e4d3fdf9d.jpg", "episode_transcript": " Linear algebra underlies much of what goes on in machine learning, and, as a subject in its own right, it has a lot of beauty. There is probably no one who has done more to evangelize the subject than Professor Gil Strang, who spent 66 years, 66, at MIT, and just recently gave his final linear algebra lecture. I had the honor to sit down with him and talk through his career, his reflection on six decades of teaching, some of the intersections between linear algebra and deep learning, and his hopes for what's to come. It was truly incredible to speak with someone who has taught so much to me and to many others. Professor Strang is an incredibly thoughtful and humble human being, and I hope this episode does some justice to his thinking. This is the Gradient Podcast, and I am your host, Daniel Beshear. If you enjoy these episodes, you can follow us wherever you're listening to this podcast episode. You can also follow us on Substack to get regular notifications whenever we release a new article, newsletter, or podcast episode. You can also find our online magazine at thegradient.pub, where we regularly publish essays by the sorts of people I interview on the podcast. And finally, if you enjoy the episode, it would mean a great deal to us all if you consider leaving us a review on whatever podcast player you're using to listen to this episode. It helps more listeners like you find what we're doing, and helps us bring in more interesting guests for you to listen to. But now, without further ado, Gil Strang. Professor Strang, I am sure many of our listeners know you as one of the most renowned educators in the world at this stage. You have been an incredible champion for linear algebra education, for mathematics education more broadly, and you've really done a service, I think, to a lot both within the machine learning field, who do have to know at least a little bit about linear algebra, and outside it. And I suppose it's kind of amazing to be speaking to you just off of your last lecture at MIT, really ending an era. But as I do with these episodes, I'd love to go back to where it all began for you. So I suppose I'd love to hear a little bit about your early academic interests, and your mathematical education, and how that kind of evolved for you into wanting to focus on teaching linear algebra. OK, I'm glad to say what I remember. And I'm happy that you saw that final lecture, which turned out to be sort of unexpectedly well known or well broadcast. Yeah, that was an adventure. Well, my whole life has been an adventure. If I go back to high school, so that was in St. Louis, and it was a boarding school. So when I arrived in ninth grade, I went to the ninth grade math teacher and said, well, I've had ninth grade math already. Do you want me to take it again? So anyway, I took an exam for that and other exams. So suddenly, I was really near the end of high school math of what was offered at that time. It's interesting to look back. So nobody in this good school taught calculus, whereas now every good high school has a class, an advanced class that reaches calculus. And what I hope is they'll also reach some linear algebra, because I think really linear algebra has become just as central as calculus and more useful for a lot of people. And that probably applies to deep learning. Yeah. So anyway, that was my high school. And then from there, I went to MIT. So I was a math major, one of a small number of math majors at that time. Now there are about 400. But that was fine. I just did ordinary stuff in college and then got a scholarship to Oxford in England. So that was actually probably that was a textbook I read there by an author named Mersky, which it just clicked with me about linear algebra. So sometimes a book clicks or a subject clicks, sometimes not. And then I was interested in sort of applied things using math. So I wrote to somebody about where should I go for grad school, maybe on the West Coast. And they suggested UCLA. And that turned out to be a good choice for graduate school. And about all this time, I was still thinking about being going into industry, being a mathematician for who knows, Bell Labs, maybe at that time. But MIT offered me a teaching job as an instructor for two years. And that was a chance to move back to Boston, which I liked. And so that started my time, my 60 whatever years at MIT. Yeah. Yeah. And then, of course, writing papers and teaching over these years, lots of different applications of linear algebra came up, solving differential equations by finite differences. And then the finite element method, that was a big activity. So that was really that was that led to my first effort to write a textbook. To write a book. So the first book was about an analysis of the finite element method. So that was a big change in my life to discover that I like teaching and also I liked writing textbooks, trying to make them clear and interesting. So at the same time, the linear algebra course, shall I carry on? This is a... Please, please go ahead. I never thought about all these things, but it all comes back now. Of course. Yeah. So it just happened when I was... So I was... Two things sort of happened at the same time. I was put in charge of the linear algebra course. It was a basic course, but it was very pure. All proofs as things were at that time, 1970s. And I thought... And of course, and the linear algebra class was only taken by a handful of math majors. The big users of linear algebra totally left out. So that was obviously couldn't continue. And so at MIT, I created a more of an applied version of linear algebra with... Thinking of how you use linear algebra, why is it so helpful? And then a lot of engineering students and other students came to that... Chose that course. And then eventually writing a textbook. And yeah, that was fun. So teaching and writing and writing research papers about linear algebra, that was the center of my years at MIT. And then finite elements was one application, which is not so far from deep learning. So one of my interests in deep learning is comparing it with a finite element method. Because both methods create piecewise functions. Functions that have a different form, maybe we divide space into a bunch of triangles. And the functions that we're interested in are linear within each piece, within each triangle. And the triangles sort of fit together. For me, that's still a highly interesting question. Why is it...how do I count for the great success of deep learning functions? What is it about those functions, which are different from the finite element engineering choice of functions that make deep learning succeed? So that would be... For me, that's the underlying math question. What kind of a learning function is created from the training data? And why does it succeed so fantastically well? I'd love to dive into that thought in more detail in a little bit. But first, maybe I do want to get a bit into your mindset really early on during the time you're describing when you were first beginning to teach at MIT and when you were writing your textbook on the finite element method. The finite element method, of course, something that would be pretty important for engineers. And so if that was a focus for you, I think that from my perspective, I can imagine you kind of saw, wait, there are these applied linear algebra methods that are really useful for engineers, but they just aren't being taught about them. When you were first sort of thinking about all of this, thinking about how do I take this very theoretical course for mathematics majors and make it more interesting for engineers? What was it that convinced you of that importance? Was it things like the finite element method where you saw these sort of clear applications that engineers could take away? What kind of convinced you that that was something that needed to be done at the time? Well, that was certainly part of it. And another area for a lot of good engineering is signal processing. So when I saw a paper and began to read papers about signal processing, those were also applied linear algebra in my thinking and some beautiful theorems about it. So that was another direction of engineering using linear algebra and not really based on calculus. So I wasn't, yeah, yeah. So the different steps in signal processing was a major example for me to say, hey, this is a really useful and not trivial, not so simple application of linear algebra. Yeah. I tended to see what, so signal processing was one very large group of engineers. Finite elements was more civil engineering, mechanical engineering, structural engineering. They're using piecewise functions in a different way. And then along came the deep learning ideas. And they succeeded incredibly well to create a learning function. And maybe I still don't feel I fully understand why I'm thinking of when ReLU is the nonlinear function that goes into it. It's an entirely new idea. To me, it's a problem of interpolating data. You have a bunch of training data, or in finite element case, the engineer has a shape of his building or structure. But so we have training data. And out of that training data, we create a function, a learning function that gives the right feedback for the training data, and then apply it to new data. And the success of deep learning is that that step works so well. Now I'm jumping forward from those early years. Yeah. But the early years were pretty exciting too, before I ever heard of deep learning. It was just another thing that luckily appeared was MIT decided to create OpenCourseWare. And at the same time, I was thinking of recording my linear algebra class. What my motive was to encourage other math professors to do it. There was a friend of mine, Giancarlo Rota, was a super lecturer in differential equations, and then he created the field of combinatorics almost single-handedly. So I always hoped his lectures would be recorded. In the end, they weren't, which I regret, but the recording of my class came at the same time that the president of MIT said okay to OpenCourseWare. So that was another very lucky coincidence. Yes. Yes, absolutely. It's interesting. So as you've pointed out, and as many of our listeners know, you were one of the first professors to put your lectures up on OpenCourseWare and really advocated for that. And of course, by now, I think you're well aware of the many, many students, including me, that you've impacted with that work. How were you when you first started putting out courses on OpenCourseWare? I guess there is, of course, this imagination that yes, let's sort of make it possible for anybody around the world to see what an education at MIT looks like and kind of get the best from our mathematics curriculum. And so I imagine that was a little bit of a mindset there, but I am curious just what your thinking was around, if you had any, around how do we sort of democratize education, take lectures that are accessible, that one can learn from, not just for the students in the classroom, but those watching via lectures as well online? Yes, that's exactly right. Well, so I really made the lectures were actually the class in progress. So for me, there wasn't a big distinction between the class I was physically teaching in front of me and whoever was watching on video at a later time. The idea was to make it clear, make the steps in mathematics clear and connected. And somehow it turned out well. Because the subject itself is so important. I don't think I'm anything special. It's just when you have a subject that people are going to use and need and it's not well realized, then I think it's almost a responsibility to help people to know what a valuable subject it is. And it was just an impossible situation previously when only a handful of math majors were taking linear algebra. That simply couldn't happen. And then of course, along came MATLAB and other computer languages that made it possible to do linear algebra on a totally different scale by using the languages that were created. So that was another step, which wasn't my step, but it was super important to have languages. So now we have in addition to MATLAB, all the other systems that your listeners know. I'd love to discuss a little bit more in detail about the applied pedagogical approach that you take linear algebra and what that entails. Maybe to give a little bit of personal experience, my introductions to linear algebra were during the Harvey Mudd Core curriculum. We had these two half semester courses. The first one was a half semester, rough seven week intro to linear algebra. The second was a mixture of linear algebra and differential equations. I remember getting to my third linear algebra class, maybe third half class, which was an intermediate course. It was taught using axelors on your algebra done right. I think personally I found that I'd lost a fair amount of intuition and I think that was for me. Our professor was kind of aware of this and I think that he did a good job closing the gap. But there was always this kind of difference between the way in which I found things may be presented in axler and then kind of, I believe our professor included a quotation to this effect on our first midterm that went something like, the theory and all is really great but at some point you just have to pick up your pen, put your head down and start multiplying matrices. And that was always, I think, taking that theory to practice with something that took a while to develop the intuition around. And so I suppose I'm curious for you, a lot of your classes that I've seen, that many others have seen, you do this wonderful job where you are really thinking through the problem out on the board as a student would and I think you've articulated your approach in that way before. I am curious how you think about a little bit closing that gap for students in terms of this is a topic that can be approached very theoretically but at some point you do just have to pick up the pen and start multiplying things as it were. Well, interesting you say start multiplying things. So actually this may serve as an example. In the very first hour of teaching linear algebra, especially as I would do it now, the one thing that's always there is how do you multiply a matrix by a vector? How do you understand A times X, matrix A times vector X? So now that linear algebra is seen pretty early, almost all students see AX, see the computation of A times X as a bunch of dot products, dot product of each row of A with the vector, the column vector X. But that's not insightful. A much more insightful understanding of a matrix times a vector is that the result is a combination of the columns of A. So from day one I'm talking about combinations of the columns of A of the matrix. In other words, so you would compute it one dot product at a time, one component at a time, but to see it, to understand it, you think of, you best to think of A times X as a, think of A as having column vectors and A times X produces a combination of those, linear combination. That's what you can do with vectors. Yes. Linear combinations and that's the fundamental concept behind A times X. And then A times B, a matrix times a matrix, that follows then column by column from A times a vector. So A times each column of B is a column of A times, of AB. So I hope my audience will forgive my talking about this such a basic thing, but that is invariably in the first lecture about linear algebra to see vectors and not just their components. Really that's linear algebra's thinking of vectors and vector and spaces of vectors. And it's that that gives you the big picture to build on. Yes. I do remember going back to that picture from your classes as I was navigating those things myself. And I think that it's interesting, I guess, just kind of returning to that basic concept and really, I guess, forming that basic intuition that kind of underlies everything else. Yeah, let me carry it to the next step because that is vector spaces. So if I multiply a matrix A by a vector, I get a combination of the columns. Now the next step is to think of doing that for all Xs. All matrices, I mean, no, we have a fixed matrix A and we multiply by all vectors X. So that gives us a whole lot of different outputs. All of them are combinations of the columns of A, that fundamental idea. So we're getting all combinations of the columns and that produces the column space. So in comes a word space, vector space, which is a bunch of vectors. And geometrically, we think of a plane, combinations of two vectors would give a plane. That's now more and more my goal in the first pages is to see, to take that next step, critical step to see that A, the matrix, a fixed matrix A times all possible vectors X gives all possible combinations of those columns of A. And that's a plane or a 3D, three dimensional vector space or an N dimensional vector space when the matrix is N by N, not necessarily N dimensional. Then we get into the question, are some columns dependent on other columns? So you're seeing what's important about independent, the idea of linear independence. So these are all the crucial first steps in understanding linear algebra. And so that's what I pushed the class to understand from the start. Yeah. And I suppose from this point, you get into a lot of concepts that do become visually intuitive over time, like span and all these things, and they do come up in very important ways when we start thinking about data, when you are doing a principal component analysis, for example, that I think a lot of people are familiar with in classical ML and having to understand the notion of orthogonality. And when you start getting into eigenvalues and all of these things. Right. That's right. And you've used the key words in all the next steps, orthogonality, it turns out for high quality numerical applications, an orthogonal basis is a big step forward over just any bunch of vectors whose combinations give the space, any bunch of vectors that span the space. When those vectors are orthogonal, you get high accuracy, you get nice understanding of the multiplication. So orthogonality eventually wins. So that comes for eigenvalues of a symmetric matrix. And then now what everybody has to learn, has to learn, is about singular values, because those apply for all matrices, not just square symmetric matrices, but all matrices, in particular matrices of data. And those matrices are not square. There's no reason why they should be. So they don't have eigenvectors, but they've got singular vectors. And those are the, they've got two sets of singular vectors, that input singular vectors, and then you multiply by a, and you get an output singular vector. And yeah, that's, so instead of ax equal lambda x, that's the eigenvalue, eigenvector picture, a times x is a multiple of x. The new important one is a times v equals sigma times u. So we have two sets of vectors, of course, because the matrix is not even square, it's rectangular. We have the inputs and the outputs are different things. But the great singular value decomposition says we can find orthogonal vectors as inputs, where the outputs also turn out to be orthogonal. That's the magic of the singular value decomposition. We have orthogonal vectors so brilliantly chosen that when you multiply by a, you get orthogonal output vectors. So that's the, so that's a big change in teaching linear algebra, which has not arrived fully, partly because if to add singular values and singular vectors to the course, the course already took a semester. And so something has to give if you're going to add in singular values and then applications like PCA, super important, and all sorts of other data analysis. So data analysis and linear algebra are coming together, but the courses have to go faster at the beginning if they're going to get to singular values in a semester. And that's the effort of my textbooks now is to move more quickly into the ideas, the first ideas of linear independence and span and the rank of a matrix and the fact that the column space has the same dimension as the row space. Wonderful, amazing fact. If those, those have to, you have to keep moving if you want to reach singular value. It is funny that you bring up singular values. I do remember early in my own linear algebra education, that was something where I got to some later courses and they were like, you've done this, right? And I think that we hadn't quite had time in the earlier algebra courses. That was totally normal. And I taught for years just doing singular values in the final few days and they wouldn't show up on the final exam. So they were just words for too much. But now that's unacceptable. You really have to explain singular values. And luckily they connect to the eigenvalues of A transpose A. So if I multiply A transpose times A, I get a symmetric matrix. And the eigenvectors of that are the singular vectors or singular vectors of A. And then, but A transpose A is different from A transpose. So we have two sets of singular vectors, just what we wanted. Yeah, that's what we wanted. Yeah. And this does come up in so many basic places. When I was getting into ML, I saw these things like low rank approximations, which of course apply singular value decomposition. Even when you're doing your very basic linear regression, you have the simple AX equals B equation that everybody kind of sees for the first time. And you're not guaranteed that A is an inverse, but then you kind of break it down with singular value decomposition and get yourself to the pseudo inverse, which I'm sure a lot of people who've kind of taken those intro ML classes are kind of familiar with. Yeah. Well, more and more, but it's the teaching of linear algebra has to be moved forward to include singular values. And pseudo inverses, I would say, that's now in the sixth edition of Introduction to Linear Algebra, the main textbook I teach from. But pseudo inverses is still marginal in the basic linear algebra course and probably not actually. Yeah. If you get the idea of orthogonality and subspaces and eigenvalues and singular values, you've done pretty well in that basic course. Before we move on to more tightly integrating what we've spoken about and some of these ideas with deep learning, I do just want to ask, since you have spent so much of your time on linear algebra, you taught more theoretically bent courses early on, I know that you do famously have a favorite matrix. I'm curious if there's any topic in linear algebra that just you find really intellectually kind of beautiful for some reason that might be just totally disconnected from its application. I see. Well, linear algebra is just a lovely subject. It's got, well, let me hammer on calculus for a moment. In calculus, you're just either taking the derivative or you're integrating. And integrating is a very interesting thing. But by comparison, the variety of ideas in linear algebra is just exceptional. You just have matrices of different sorts. So of course, for symmetric matrices, I think of those as the king of linear algebra. Kings, and then maybe orthogonal matrices as the queens. And then, but a whole lot of others, for example, banded matrices to understand matrices that have or low rank matrices you mentioned. These different categories of matrices, they all have their own applications, their own importance, their own theory, and connecting them together is just great mathematics. That's really what mathematics is, is seeing the connections between different ideas, ideas that look different but are nevertheless somehow connected. So and orthogonality is important for accurate computations, but bandedness or sparseness is important for fast computation. So it's a battle between the two, a battle between keeping, so orthogonal vectors are as independent as you can get, and sparse vectors are as efficient and meaningful as you can get. And there is always a compromise, and deep learning has found a fantastic new direction for compromise, for achieving both. Yeah. Yeah. For me, the conclusion for calculus is I think that calculus needs a greater development of piecewise polynomial functions or piecewise linear functions. Yeah, the space of piecewise linear functions, that's an interesting link between calculus and algebra. Let's start to work our way towards deep learning in a little bit more detail. And so one thing we were talking about a little bit earlier before we started recording was some of the different pedagogical approaches to thinking about understanding deep learning. You have the kind of top-down, let's begin with the code kind of approach of Jeremy Howard and fast.ai, you create a deep network and seven lines of code, you get it to work, and then you kind of start popping off the layers, and you also have the bottom-up approach. Let's actually dig down into the basics. Let's understand linear algebra from the very basic ideas and kind of work forward and build up the tools. And I suppose there's always kind of this blend, you want room for both. Of course, Jeremy Howard doesn't completely ignore linear algebra. He sort of progressively brings it in. And I think from the other approach as well, there are ways to kind of bring in concepts from later. But I'm curious how you think specifically about for students who are trying to approach the ideas in deep learning. And before we get into some of these questions of why does it work, just getting a grip on what is actually happening in the first place, how do you think about effectively communicating those ideas? Well, let me say that I would like to do more and better with communicating those ideas. To what extent can this tremendous success of deep learning be reflected in high schools or in early math courses? Certainly the idea of piecewise functions is crucial here. That has to be a big part of the magic of magical success of deep learning. And the fact of composition that you start with the idea of a neural network that has layers and you create the functions that you create are f of x is f1 of f2 of f3 of f4 of fL of x. You're the famous functions that we use the chain rule to find the derivative. So the chain rule is like the fundamental calculus idea for deep learning, if I'm allowed to step outside what I know and just talk about deep learning. Yeah, so for me, deep learning hinges on the success of the fast route to complexity, the fast route to generalization is f1 of f2 of f3 of f4. Those functions get complicated very quickly and powerful very quickly. And then the effect of the non-linearity, the effect of ReLU producing piecewise functions. So it's a combination of for me, the two key math ingredients are the composition of multiple layers of a new function at each layer and the ReLU, the non-linearity that turns linear into piecewise linear. And that's sort of a, you could say a high level view of the mathematics of deep learning. I really do want to ask from your perspective thinking about neural networks as, you know, in the way you do, I think that there are a lot of ways people kind of articulate this. We call neural networks universal function approximators or kind of flexible templates for creating functions. And there's this within the field, as I'm sure you've probably seen, there are these lingering questions. They have been for a while about why neural networks seem to generalize so well. It's very non-intuitive. We had these theoretical tools about VC dimension or Rademacher complexity that we were kind of able to take the hammer to some of the old methods. But then you apply them to deep learning and you get these kind of degenerate statements that really don't mean anything. And so there's this ongoing struggle about, well, these things should really just be memorizing. But it seems like even when I train deep neural networks, one kind of important result recently was you take a deep neural network and if you actually train it to a point where it overfits, where the test loss is much higher than the train loss, and if you actually keep training it, then it kind of starts to generalize instead of just memorizing. Right. That double descent curve where at the beginning you get the error goes down when you obviously when you use more functions, but then if you try too many, it goes up again and then it goes down again when you have more. So generalization. Yeah, that's been the key theoretical question is to explain that success. And I think I'm not probably not the person who's really up to date, fully up to date on the theory of generalization, but I'm certainly recognizing that that's a fundamental question and that it's gradually being answered. Along with that, another sort of question I have for you from your perspective is, along with the way in which we think about neural networks, one sort of articulation you hear among people who want to make points about the powers and limits of these algorithms is sort of articulating them in their mathematical form. So Francois Chollet often talks about deep neural networks as manifold manipulators, which I think is one kind of geometric way to put it. And he does not deny that this is an incredibly powerful thing to be able to do in very high dimensions when you have lots of data. But the point he really wants to make with that, of course, is that there's some kind of limit here because a lot of people who were sort of thinking about this deep learning connection as paradigm early on wanted to figure out, well, maybe human cognition kind of looks like this. And so I suppose I am curious for you as somebody looking at this through a more mathematical lens how you think about the powers, the fundamental maybe limitations of neural network-based methods. Well, that's a good question and probably not one to which I have a good answer. It's, well, you've already interviewed people like Yoshua Bengio and others who have a deep understanding of the ideas, how they fit together into such a successful thing. That's interesting for me to ask the question, did it have to happen that deep network, deep learning evolved as it did? Was that the only, have we now got a construction of approximating functions to fit the data but still be stable if the data moves a little? Or could we have done it in an entirely different way? Or the big picture, of course, I agree, it's quite sensible to be happy with the success that we have using ReLU and the codes that are established. But to understand why it is that they work and are there other possibilities that could achieve that is just a natural question to ask, I guess. And not a question I would know an important answer. As I was trying to say, I'm maybe especially interested in the question of how to teach deep learning in high school, for example. Because everybody reads in the paper now about Chachi P., of course, and all other ideas that are tremendously successful. But can we explain why or even give a sense of why they work in high schools and also give the students a chance to see them work, see them succeed? Yeah. So if any of your listeners have successfully embarked on teaching, explaining these ideas to an early education level, I'd be interested to know. It's funny you mentioned that I actually did participate for a little while just kind of part time because I thought it might be fun, this group that was working on establishing a curriculum to teach high school students about the basics of deep learning. Tell me about that group. Yeah. Sure. Yeah, I guess I'm giving them a little bit of a plug here. They're called InSpirit AI. And they were started by a bunch of students. I think a lot of them were at Stanford kind of initially. The way we sort of taught these basics, I think, was well, the students were, I think the students we were teaching were often pretty advanced in mathematics. And so it was kind of scaled based off of how much coding experience the students might have had. But I think that a lot of the kind of basic intuitions we tried to convey to them, like one of the first things we went into was how a convolutional neural network worked. And I think you can really kind of show a good intuition of what is going on in a convolutional layer. And I think that the key that kind of worked for us and for the students was these ideas are very visually intuitive. And so we were kind of able to display that notion of here's a feature detector that you can learn. What a Sobel kernel kind of looks like when you apply that. So we made use of, I think, a combination of maybe a bit of a more like let's get down and multiply a couple of matrices here in a very small example kind of setting and combining that with a lot of visualization. And I found, I think for our case, that tended to be a pretty powerful way of conveying these ideas. And at least in maybe I was just very lucky, but at least in my experience, the students kind of seem to get what was going on. Oh, well, that sounds like the success that I'd like to see. And I hope you'll, when our interview is over, send me a message about where I should look and learn. Yes, yes, absolutely. I'd love to dig in a little bit into. So as you've mentioned, and many know you spent 66 years teaching at MIT. And so this is really a period of time that saw, I think, a lot of change, as we mentioned in the importance of and the attention paid to linear algebra by people who were not you. And I suppose I'm curious to hear a little bit about, I can imagine that there has probably been a lot of consistency in your pedagogical approach. But I guess I'd love to hear you talk a little bit more about what changed, what stayed the same about how you approached teaching linear algebra over time. Okay, thank you. Well, I guess my teaching evolved and continues to evolve. You know, the sixth edition of Introduction to Linear Algebra, which is now my textbook, has a different start from the fifth edition. And it includes now deep learning and optimization chapters at the end of the book. So the result is a different start and a different finish for a basic linear algebra textbook. Yeah, so and of course, recognizing the limitations of one semester. You can't do everything. But well, let me just go back to one algebra idea, which turned out well. And it came from the very, it was there from the very first linear algebra book that I worked on and the linear algebra course was the idea of four fundamental subspaces. I don't know if you know those by name. So that, yeah, the row space, the combinations of the rows, the column space, the combinations of the columns, the null space, the solutions to AX equals zero. And the fourth one is the solutions to A transpose X equals zero. The roles of A and A transpose are fundamental to linear algebra, fundamental that you have the same numbers are appearing in the rows of the matrix as in the columns, but they're different vectors. They're different length. Everything looks different. But of course, the fact that the same numbers are going into them means that they are closely related. Yeah. So that's that's teaching the linear algebra part. Yeah, I don't feel that I have a great picture of how to teach deep learning. Well, partly because I think we still have some things to understand about generalization and deep learning. But I'm interested to learn more about your in spirit AI. Because if it's exceeded for you, that's a very good sign. Yeah, I would have to flag. I think that we probably what we got across for the students was a lot of the intuitions with a little bit of mathematics there, which is what you'd expect and hope for at a high school level. Yeah, yeah. I think it sounds you were on the right track. Was there a Stanford faculty who took a part in that or who who kind of led led the effort? Oh, sure. Yeah. So it was mostly an initiative that was led by students. And I think right now it's more positioned as a business, the way that they're kind of running it. But yeah, so I think I no longer have access to the curriculum materials. But at the very least, I did like what they were doing. And I think that, like it actually did a pretty decent job at communicating the ideas. That's a good recommendation. And and it would be accessible to good high school students or good college students. I would think so. Yeah, to get high school students. Absolutely is the goal. OK, OK. Excellent. Well, maybe we can talk more. So for me, the and of course, as we know, deep learning, different kinds of networks, different applications are continuing to come forward. And the whole subject is just inspiring to see all that happen. I think so, too. I think this would be a good place for maybe a closing question. And I suppose the most appropriate one would be, as we've mentioned, you you recently gave your final lecture at MIT and I and many others watched it. And I just love to hear you maybe reflect a little bit on the 66 years you've been teaching and kind of now that, of course, I imagine you kind of want to stay as engaged as you can, even if you're not officially teaching from the sound of it. I suppose I'm just curious about how how these these decades of your life spent as an educator you spent really championing linear algebra and mathematics education, how that's kind of affected you personally. I see. OK, let me put in just one thought. Now that those all those years are over, I'm retired. I won't be teaching a class at MIT. But my thoughts are still working. And so one natural question for me is whether high school algebra needs some new thinking. And I have to confess that I don't know anything about how high school algebra is taught. I have everything to learn. And I'm hoping to ask friends and learn more. And maybe you're maybe this recording will bring some some thoughts from from the audience from listeners about and they'd be very welcome. Just Gil Strang at Gmail dot com. But I hope to learn more about about that direction. Yeah, so but your your question was more about the 60. Well, three years of learning and 63 years of teaching was it was a 66 total. And of course, wonderful years. Teachers are very lucky people. Yeah, so I I feel that the linear algebra has and not just because it's so important that that had to that growth and development had to evolve. And it did. And it's just a pleasure to look back at to see to see it all happen. And thanks to deep learning, there are more questions that kind of link calculus with linear algebra. Again, piecewise linear is a link class of functions that connect those two great subjects. So there's more to come at my thought at that age 88. There's more to come. Well, I and I hope everybody listening to this are very excited to see what is what is yet to come. Professor Strang, I do want to thank you for being so generous with your time and for speaking with me today. It was really an honor. Oh, thank you. It was a pleasure. And that is a wrap, my friends. As I mentioned at the start of the episode, you can subscribe to the gradient on sub stack to receive not just this podcast, but also our articles and newsletters directly to your email. You can also visit us at the gradient dot pub, where you'll find all of that as well as more information about the gradient and how you could even contribute if you're interested. And finally, if you enjoyed this episode, we would really appreciate your feedback. If you'd like to leave a comment or review, we'd love to know how we can make this series more interesting and informative to you. And with all that, I'll leave you until the next episode."}, "podcast_summary": "In this podcast, Professor Gil Strang, renowned for his work in linear algebra education, reflects on his career and the intersections between linear algebra and deep learning. He discusses his early academic interests, his mathematical education, and how his focus shifted to teaching linear algebra. He emphasizes the importance of teaching the fundamental concepts of linear algebra, such as vector spaces and orthogonality, and how they relate to applications in signal processing, finite element methods, and deep learning. Professor Strang also discusses the need to incorporate singular values and singular vectors into the curriculum and explores the question of why deep learning networks generalize so well. He concludes by expressing his interest in teaching deep learning at the high school level and invites feedback and insights on how to approach it effectively. Overall, Professor Strang highlights the beauty and importance of linear algebra in mathematics education and its application in various domains, including deep learning.", "podcast_guest": " Gil Strang", "podcast_highlights": "Some key moments in the podcast include:\n\n- Professor Gil Strang spent 66 years at MIT and recently gave his final linear algebra lecture.\n- Linear algebra underlies much of what goes on in machine learning.\n- Professor Strang has been an advocate for linear algebra education and has focused on teaching linear algebra throughout his career.\n- He discusses his early academic interests, his mathematical education, and how he became interested in teaching linear algebra.\n- Professor Strang talks about the importance of understanding matrix multiplication as combinations of columns and the concept of vector spaces.\n- He mentions the connections between linear algebra and deep learning, including the use of piecewise linear functions and the composition of multiple layers in a neural network.\n- Professor Strang reflects on the changes in teaching linear algebra over the years and the need to include topics like singular values and pseudo inverses.\n- He discusses the challenge of teaching deep learning at a high school level and the need for more exploration in this area.\n- Professor Strang reflects on his 66 years of teaching and the impact of linear algebra on his career. He also expresses his continued interest in exploring the connections between calculus and linear algebra."}